{
  
    
        "post0": {
            "title": "Artifical Intelligence And Iq Tests",
            "content": "Writing Code to Pass IQ tests . Background . In the summer of 2016 I started my Master’s degree in computer science. To be honest, I really didn’t know what I had gotten myself into. Luckily I started my degree with one of the top rated courses, Knowledge Based Artifical Intelligence - Cognitive Systems, run by the talented David Joyner. Throughout the semester I designed and built software that passed the Raven’s Progressive Matrices. . The idea behind using Raven’s Progressive Matrices (RPM) is that it is a great test of fluid intelligence - the ability to generalize and reason abstractly. A lot of research in deep learning currently focuses solely on identifying objects (computer vision), but this is only useful because there is normally a human doing the reasoning after the neural network has done the recognition. To build a generalized intelligence we need to consider how to architect software to perform visual reasoning. . The project was performed in three iterations, with the first iteration only needing to pass 2x2 RPMs . Verbal reasoning . For the first iteration I used a verbal reasoning method - the visual matrix was accompanied by a text representation that essentially solved the ‘correspondence problem’ for me - the problem of identifying which object in one box corresponded to which object in another box. The basic steps I took were as follows: . Ingest/Parse the problem A very simple ingestion of the data to convert the RPM into more easily manipulated data structures. Angle and size values were converted into integers, and alignments were converted into booleans. . | Get objects from each figure This was a further round of parsing to represent the positional aspects of the objects with respect to each other. . | Calculate transforms In this step I calculated the simplest transforms that converted the top left objects into the top right objects. . | Compare all possible answer transforms (generate and test) For every single transform that was present in the A =&gt; B transform pipe that was also present in the C =&gt; answer transform pipe, the similarity score of that total transform was incremented. Thus each possible answer box was represented by a transform pipe with a score of how similar it was to the A =&gt; B transform pipe, allowing me to select the answer with the highest score. . | This agent had difficulty with identifying reflection transformations, instead preferring to view them as rotations, as well as considering vertical relationships (the relationship between A and C, and B and #) . Visual reasoning . For the next two iterations I had to move to visual representations of the problem - I was losing too much information in trying to represent a complex environment with text descriptions. At its core, the visual agent uses pixel-based heuristics to pick the most likely answer. It first converts all figures to numpy arrays of pixels. It then captures the changes in the arrays between figures in the problem. It then uses a generate and test method to compare the transform required for each candidate answer with the training transforms, picking the candidate answer with the most similar transform. . What does any of this tell us about human cognition? . This project has made it abundantly clear to me how holistic and multi-layered the human method of reasoning is. The pac-man problem is an excellent example - humans do not consider rotations, reflections, black pixel ratios or overlap, or indeed any transformations between the figures. Rather, the problem is solved because the correct answer produces the corners of a square that lies between A, B, C, and #. The figures together necessitate answer 3 as the correct response because it completes the gestalt. My agent does not consider anything greater than its current area of focus (a single figure and its immediate neighbor). Thus, the human reasons at multiple levels of abstraction, and has multiple strategies for solving RPMs. These strategies can be based on simple transformations, arithmetic (number of points on an object), semantics, or any other number of strategies. . Clearly, though, humans do not explicitly count pixels or determine overlap between figures. However, at a higher level of processing, you could argue that they do employ some kind of method similar to pixel counting - what my agent sees as an increase in the number of pixels, a human solver sees in more semantic terms (e.g. the square has got bigger or a new object appeared). The pixel overlap metric could be considered to be a very primitive part of a solution to the correspondence problem - where my agent sees a group of pixels that occupy the same space in two different figures, a human would see objects that correspond to one another based on their position in the figures. . One aspect in which my agent is clearly different from human solvers is that it is incapable of generating its own answer and then checking if this answer is available in the list of choices. In the pac-man example, the human generates an appropriate answer and then tests it against all possible answers. Only for more difficult tests is each possible answer is viewed in turn to see if it fits - thus the human is very capable of switching the form of generate and test that is being used. . It is clear to me that human solvers use multiple methods to solve problems. While my verbal agent represents the higher-level reasoning of a human solver, my visual agent represents the very low-level processing that the visual cortex would do. There is no reason that these two processes could not be simultaneously run. In fact, if a Raven’s problem that was simply random pixels (with no definable shapes) was presented to a human and the human were asked to describe their reasoning, surely their reasoning would involve picking an answer based roughly on black pixel count and black pixel overlap? . Thankfully, the voting nature of my project made it easy to extend with different strategies. Some of the more challenging problems were sufficiently tricky that I had to consider what relationships I saw in the problem, and then try and code a way for the agent to also recognise them. In this way, the agent multiplies the votes resulting from a certain relationship, if the salience of that relationship is sufficiently high. For example, if a horizontal XOR progression is seen in the figures at a high significance level, all votes from this relationship are multiplied. If a diagonal relationship is very significant (i.e. all the diagonal relationships are close to 100%, while the horizontal and vertical relationships are not), then diagonal votes are multiplied. All of these processes work simultaneously, so the multipliers may be changed for all the relationships. What falls out of the calculations at the end is simply an array of votes for each possible answer. In this way, some relationships simply ‘pop’ out at the agent, in the same way that some relationships just jump out at the human viewer. The multipliers are simply a way of scoring the salience of relationships to humans. . I found the last problem most interesting. I initially described its solution to myself by giving each object a meaning - an up triangle represented +1, and a down triangle represented -1. Simple addition and subtraction then gave the answer. I initially thought that there was no way my agent would be able to pick the answer. I only consider ‘up’ to be positive because that’s the way humans commonly represent numbers, and I only consider the triangle to be pointing in a certain way because I am used to seeing directions and arrows drawn like this. My agent has no background semantic knowledge to apply. I was then surprised to see my agent get the right answer using only pixel based heuristics. This pretty much summed up the series of projects in this course for me - the ability of humans to very quickly use different strategies to see patterns and apply meaning to things, and the ability of AI agents to achieve complexity from very simple precepts. Clearly, the agent does not understand the meaning I have applied to the figures, and yet it achieves the same result from very simple heuristics. .",
            "url": "https://johnptmcdonald.github.io/blog/2020/03/12/Artifical-Intelligence-and-IQ-tests.html",
            "relUrl": "/2020/03/12/Artifical-Intelligence-and-IQ-tests.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://johnptmcdonald.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johnptmcdonald.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is John McDonald, and I am currently a senior software engineering instructor at Fullstack Academy in New York City. I build things with Javascript, React, Redux, Angular, Node, Express, Python, and PyTorch. .",
          "url": "https://johnptmcdonald.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}